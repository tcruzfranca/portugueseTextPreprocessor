{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento\n",
    "\n",
    "\n",
    "Algumas ferramentas de pré-processsamento de texto usando técnicas de processamento de linguagem natural.\n",
    "\n",
    "Este notebook é um guia! Você vai precisar rodar localmente o código, mas não pule essa etapa, é muito importante entender como funciona a ferramenta para tratar seus dados corretamente.\n",
    "\n",
    "Os métodos disponíveis podem ser chamados em outro código que você criar.\n",
    "\n",
    "Para ter acesso clone o repositório: git@github.com:tcruzfranca/portugueseTextPreprocessor.git\n",
    "\n",
    "Você vai precisar dos seguintes arquivos para usar os métodos listados nesse notebook:\n",
    "1. Preprocessor.py\n",
    "2. removeStopWords.py\n",
    "\n",
    "Não esqueça de conferir localmente e comparar com o notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "As *stop words* são palavras curtas que geralmente não alteram de forma substancial o sentido de um texto ou uma frase. Em nossa ferramenta, temos um arquivo \"txt\" com uma lista de palavras do português(do Brasil) que podem ser consideradas *stop words*. Caso você queira adicionar ou retirar alguma, basta editar esse arquivo(stopwords_pt.txt).\n",
    "\n",
    "Abaixo nós iremos usar algumas funções para tratar as stopwords, e mais adiante no código, removê-las quando necessário.\n",
    "\n",
    "Nem sempre é interessante seguir essa abordagem. Por exemplo, caso seu produto final seja uma análise de sentimento, não é muito útil remover palavras como *\"bem\"* e *\"não\"* que são curtas, mas podem ser cruciais para o sentido do texto.\n",
    "\n",
    "O ideal é remover as *stop words* apenas quando elas não possuem relevância no texto para o seu objetivo(classificação de temas por exemplo). Existe um artigo rápido e interessante sobre isso(achei apenas em inglês):https://medium.com/@wilamelima/why-is-removing-stop-words-not-always-a-good-idea-c8d35bd77214\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _list_de_stopWords(arquivo):\n",
    "    \n",
    "    lista = []\n",
    "\n",
    "    for stop_word in arquivo:\n",
    "        stop_word = stop_word.lower()\n",
    "        stop_word = stop_word.replace('\\n','')\n",
    "        stop_word = stop_word.replace('\\r','')\n",
    "        lista.append(str(stop_word))\n",
    "\n",
    "    return lista  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def listStopWords(language=\"pt-br\", f=\"stopwords_pt.txt\"):\n",
    "\n",
    "    arquivo = \"\"\n",
    "    if language == \"pt-br\":\n",
    "        arquivo = open(f)\n",
    "    else:\n",
    "        print(\"Inclua a lista e altere o método listStopWords em removeStopWords\")\n",
    "\n",
    "    return _list_de_stopWords(arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bibliotecas\n",
    "\n",
    "\n",
    "### *removeStopWords*\n",
    "\n",
    "No arquivo removeStopWords.py estão as funções que fizemos acima! No notebook a biblioteca está comentada, mas não faça isso no arquivo local.\n",
    "\n",
    "## ptstemmer\n",
    "    \n",
    "É a biblioteca que irá nos fornecer os algoritmos de stemização. O OrengoStemmer apresentou um desempenho superior aos outros e por isso foi definido como padrão, mas você pode usar os outros caso queira.\n",
    "\n",
    "Stemização é um processo de redução das palavras flexionadas sem se preocupar com preservar o sentido original delas, as palavras são reduzidas ao seu \"stem\" removendo acentos, sufixos, prefixos e outros morfemas.\n",
    "\n",
    "E como isso é útil? Se seu corpus textual for muito grande e você precisa diminuir a dimensão dele, a stemização irá retornar palavras muitas vezes idênticas(escritas de forma diferente, ex: \"Porta\" e \"poRtA\") ao seu stem,\n",
    "reduzindo a quantidade de palavras e mantendo o corpus.\n",
    "\n",
    "#### Instalação\n",
    "\n",
    "    1) Clone esse repositório para qualquer diretório do seu computador: https://github.com/edumangabeira/ptstemmer\n",
    "    \n",
    "    2) Pelo terminal, vá até a pasta onde está a versão para Python ptstemmer/Python ou clique com o botão direito na pasta e abra o terminal\n",
    "    \n",
    "    3) Digite no seu terminal python3 setup.py install(ou apenas python setup.py dependendo da versão na sua máquina) \n",
    "\n",
    "\n",
    "## unicodedata\n",
    "\n",
    "Ela vai nos garantir que o nosso formato padrão(utf-8) se mantenha por todo o texto.\n",
    "\n",
    "## re\n",
    "\n",
    "Para remover alguns caracteres especiais e padronizar os tweets vamos fazer uso de expressões regulares(regex). Mais à frente serão explicadas as escolhas de remoção.\n",
    "\n",
    "Se conhece pouco do tópico e está interessado em saber mais, esse guia rápido pode ser de ajuda: http://mindbending.org/pt/brincando-expressoes-regulares-no-python\n",
    "\n",
    "também há um canal no youtube excelente falando sobre isso(e mais sobre python em geral), mas o conteúdo é em inglês: https://www.youtube.com/watch?v=sa-TUpSx1JA\n",
    "\n",
    "\n",
    "## Spacy\n",
    "\n",
    "Essa biblioteca oferece diversas ferramentas de processamento de linguagem natural, entre elas a lematização, processo similar ao de stemização, mas em vez de reduzir as palavras sem se importar com preservar a semântica, lematizar faz com que a palavra seja reduzida ao seu radical.(ex: \"comemos\", \"comeram\", \"comem\": todos podem ser reduzidas a \"comer\". \n",
    "\n",
    "Assim como o processo de stemming, aqui o objetivo é diminuir a variação morfológica do texto, mas de forma bem mais sofisticada por levar em conta o contexto em que a palavra foi inserida e a classificação do morfema, o único problema é tornar confiável a acurácia para classificar um morfema. \n",
    "\n",
    "Atualmente o Spacy conta com um modelo da língua portuguesa baseado em textos jornalísticos e artigos da wikipédia, ou seja, seu uso para tratar discursos na primeira pessoa pode encontrar muitos erros.\n",
    "\n",
    "Apesar de ser um procedimento complicado você pode treinar seu próprio modelo. Os desenvolvedores disponibilizam um tutorial com todos os recursos da biblioteca que se enconta em : https://course.spacy.io/\n",
    "\n",
    "   #### Instalação\n",
    "       1) Instalação rápida\n",
    "       \n",
    "       pip install -U spacy\n",
    "   \n",
    "       2) Caso queira instalar num ambiente virtual:\n",
    "           \n",
    "           - python -m venv .env\n",
    "           - source .env/bin/activate\n",
    "           - pip install spacy\n",
    "           \n",
    "       3) Se você tiver o anaconda:\n",
    "         \n",
    "         conda install -c conda-forge spacy\n",
    "      \n",
    "   #### ATENÇÃO\n",
    "     \n",
    "   Também é preciso baixar o modelo da língua portuguesa:\n",
    "   python3 -m spacy download pt\n",
    "      \n",
    "   para mais detalhes visite a página:  https://spacy.io/models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    @authors\\n        Tiago Cruz de França\\n        Eduardo Freire Mangabeira\\n    @since\\n        03-23-2015\\n    @version \\n        1.0.0\\n    @see\\n        https://github.com/tcruzfranca/annotatedDatasetBrazilianProtests\\n        This is a specific version made for turn easy the retrival of tweets from the Brazilian protests Golden Dataset.\\n        For a more generic version for retrival of tweets by id, access https://github.com/tcruzfranca/scripts.\\n    License (BSD 2): Available in https://github.com/tcruzfranca/annotatedDatasetBrazilianProtests/blob/master/LICENSE.txt.\\n    \\n    description \\n                This code is useful for retrieval tweets usin list of tweet's IDs. Such list must be a file in which each ID is in a different line.\\n                In the end of this file you can see the main function and the instructions for set correctly the configurations needed.\\n    \\n    Please access the git address and cite at least one: our paper or the git account if we want use this script.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "# import removeStopWords\n",
    "import spacy\n",
    "#import csv\n",
    "from unicodedata import normalize\n",
    "from ptstemmer.implementations.OrengoStemmer import OrengoStemmer\n",
    "from ptstemmer.implementations.SavoyStemmer import SavoyStemmer\n",
    "from ptstemmer.implementations.PorterStemmer import PorterStemmer\n",
    "\n",
    "'''\n",
    "    @authors\n",
    "        Tiago Cruz de França\n",
    "        Eduardo Freire Mangabeira\n",
    "    @since\n",
    "        03-23-2015\n",
    "    @version \n",
    "        1.0.0\n",
    "    @see\n",
    "        https://github.com/tcruzfranca/annotatedDatasetBrazilianProtests\n",
    "        This is a specific version made for turn easy the retrival of tweets from the Brazilian protests Golden Dataset.\n",
    "        For a more generic version for retrival of tweets by id, access https://github.com/tcruzfranca/scripts.\n",
    "    License (BSD 2): Available in https://github.com/tcruzfranca/annotatedDatasetBrazilianProtests/blob/master/LICENSE.txt.\n",
    "    \n",
    "    description \n",
    "                This code is useful for retrieval tweets usin list of tweet's IDs. Such list must be a file in which each ID is in a different line.\n",
    "                In the end of this file you can see the main function and the instructions for set correctly the configurations needed.\n",
    "    \n",
    "    Please access the git address and cite at least one: our paper or the git account if we want use this script.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos\n",
    "\n",
    "\n",
    "Temos diversos métodos na classe do Pré-Processador que a seguir serão comentados em mais detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replaceNonASCIIcharacter(self, text, codif='utf-8')\n",
    "\n",
    "Remove todos os caracteres que não estão na tabela de codificação ASCII e não alfa-numéricos\n",
    "(ex: bullets, travessões, caracteres acentuados, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removeNonAlphaNumericValues(self, text)\n",
    "Remove todos os caracteres que não são alfa-numéricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textFilter(self, text)\n",
    "Este método foi criado pensando especificamente no texto de um Tweet, ele remove retweets, menções, url, espaços multiplo, tabulações, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replaceTwoOrMore(self, text)\n",
    "Remove caracteres repetidos de um texto.\n",
    "(Falta incluir exceções: ss, rr, ee e oo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removeSmallWords(self, tweet, minSize = 2)\n",
    "Remove qualquer string menhor ou igual ao tamanho mínimo informado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _getStopWords(self,language=\"pt-br\")\n",
    "Nesse Método já estamos chamando aquele outro código mencionado anteriormente(removeStopWords.py). Ele inicia o atributo stopWords da classe Preprocessor com as stop words do arquivo texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopWordsWithoutNonASCIICharacteres(self,onlyASCII = True,language=\"pt-br\")\n",
    "Carrega stop words removendo todos os caracteres que não estão na tabela de codificação ASCII."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_stopWords(self, tweet, language=\"pt-br\", list_stopWords=[]):\n",
    "Outro método feito pensando no Twitter, mas se estende para qualquer texto genérico.\n",
    "Remove stopwords do tweet com base na lista_stopWords criada em lista_de_stopWords(arquivo).\n",
    "Usado depois de remover pontuacoes do tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _getStemmerObject(self, language=\"pt-br\", approach=\"orengo\")\n",
    "Inicializa o Stemmer a ser utilizado. O stemmer padrão e recomendado é o Orengo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming(self,palavras,language=\"pt-br\", approach=\"orengo\")\n",
    "Stemiza uma lista com uma ou mais palavra e retorna a lista com as palavras stemizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemmingFrase(self,frase,language=\"pt-br\", approach=\"orengo\")\n",
    "Stemiza uma frase inteira e retorna o texto stemizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizePhraseWithoutStopwords(self, frase, language=\"pt-br\"):\n",
    "Lematiza uma frase ignorando stopwords, os lemas são salvos(individualmente) numa lista em que estão indicadas as identidades de cada morfema.\n",
    "Para saber mais sobre o que significa cada identidade: https://spacy.io/usage/linguistic-features#pos-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizePhraseWithoutStopwordsandPOS(self, frase, language=\"pt-br\")\n",
    "Lematiza uma frase ignorando stopwords e a análise morfológica das palavras(Part-of-Speech Tagging - POS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizePhrase(self, frase, language=\"pt\")\n",
    "Lematiza uma frase e inclui os lemas numa lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopWords = []\n",
    "        self.stopWordsOnlyASCIICharacteres = False\n",
    "        self.stemmer = OrengoStemmer()\n",
    "        self.stemmerType = \"orengo\"\n",
    "\n",
    "    def removeNonAlphaNumericValues(self, text):\n",
    "\n",
    "        text = re.sub('[^a-zA-Z0-9@]',' ',text)\n",
    "        text = re.sub('\\s{2,}',' ',text)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def replaceNonASCIIcharacter(self, text, codif='utf-8'):\n",
    " \n",
    "        return normalize('NFKD', text).encode('ASCII','ignore').decode(codif)\n",
    "\n",
    "\n",
    "    def textFilter(self, text):\n",
    "   \n",
    "        text = text.lower()\n",
    "        text = re.sub('([RT]|[rt]*\\s*@[a-z]+)|(http([!a-z]|[^ \\t\\n\\r\\f\\v]*))|([^a-zA-ZÀ-ú0-9\\s])',' ',text)\n",
    "        '''\n",
    "        o intervalo À-ú para englobar todos os acentuados, ou ainda À-Ú e à-ú caso se queria só maiúsculas ou minúsculas.\n",
    "            [[:lower:]]\t[a-zà-ú]\n",
    "            [[:upper:]]\t[A-ZÀ-Ú]\n",
    "            [[:alpha:]]\t[A-Za-zÀ-ú]\n",
    "        '''   \n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "    def replaceTwoOrMore(self, text):\n",
    "        \n",
    "        pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "        findings = pattern.findall(text)\n",
    "        #print (findings)\n",
    "        return pattern.sub(r\"\\1\", text)\n",
    "\n",
    "    def removeSmallWords(self, tweet, minSize = 2):\n",
    "        \n",
    "        palavras = tweet.split()\n",
    "        palavra = \"\"\n",
    "        for pal in palavras:\n",
    "            if len(pal) > minSize:\n",
    "                palavra += pal+\" \"\n",
    "        return palavra.strip()\n",
    "\n",
    "\n",
    "    def _getStopWords(self,language=\"pt-br\"):\n",
    "        \n",
    "        if len(self.stopWords) == 0:\n",
    "            self.stopWords = removeStopWords.listStopWords(language)\n",
    "        return self.stopWords\n",
    "\n",
    "\n",
    "    def stopWordsWithoutNonASCIICharacteres(self,onlyASCII = True,language=\"pt-br\"):\n",
    "        \n",
    "        if (self.stopWordsOnlyASCIICharacteres and onlyASCII):            \n",
    "            pass\n",
    "        elif (not(self.stopWordsOnlyASCIICharacteres) and onlyASCII):\n",
    "            stop_words = self._getStopWords(language)\n",
    "            self.stopWords = [self.replaceNonASCIIcharacter(str(i.encode(\"utf-8\"))) for i in stop_words]\n",
    "            self.stopWordsOnlyASCIICharacteres = True            \n",
    "        elif (self.stopWordsOnlyASCIICharacteres and not onlyASCII):            \n",
    "            self.stopWords = []\n",
    "            self.stopWords = self._getStopWords(language)\n",
    "            self.stopWordsOnlyASCIICharacteres = False\n",
    "        else:\n",
    "            self.stopWords = self._getStopWords(language)\n",
    "        \n",
    "        return onlyASCII\n",
    "\n",
    "        \n",
    "\n",
    "    def remove_stopWords(self, tweet, language=\"pt-br\", list_stopWords=[]):\n",
    "        \n",
    "        listStopWords = []\n",
    "        if len(list_stopWords) == 0:     \n",
    "            listStopWords = self._getStopWords(language)\n",
    "        else:\n",
    "            listStopWords = list_stopWords\n",
    "\n",
    "        listTokensTweet = tweet.split()\n",
    "        text = ''\n",
    "        for i in listTokensTweet:\n",
    "            #print(i) \n",
    "            i.strip()    \n",
    "            if i not in listStopWords:\n",
    "                text += i+' '\n",
    "        return text.strip()\n",
    "    \n",
    "    def _getStemmerObject(self, language=\"pt-br\", approach=\"orengo\"):\n",
    "\n",
    "        if (approach != self.stemmerType):\n",
    "        \n",
    "            self.stemmerType = approach\n",
    "            if approach == \"orengo\":\n",
    "                self.stemmer = OrengoStemmer()\n",
    "                \n",
    "            if approach == \"porter\":\n",
    "                self.stemmer = PorterStemmer()        \n",
    "\n",
    "            if approach == \"savoy\":\n",
    "                self.stemmer = SavoyStemmer()        \n",
    "\n",
    "        return self.stemmer\n",
    "            \n",
    "\n",
    "    def stemming(self,palavras,language=\"pt-br\", approach=\"orengo\"):\n",
    "\n",
    "        stemmer = self._getStemmerObject(language, approach)\n",
    "        if type(palavras) is str:\n",
    "            return stemmer.getWordStem(palavras)\n",
    "\n",
    "        lista = []        \n",
    "        for palavra in palavras:\n",
    "            lista.append(stemmer.getWordStem(palavra))\n",
    "\n",
    "        return lista\n",
    "\n",
    "\n",
    "    def stemmingFrase(self,frase,language=\"pt-br\", approach=\"orengo\"):\n",
    "        palavras = frase.split()\n",
    "        palavras = self.stemming(palavras)\n",
    "        text=\"\"\n",
    "        listStopWords = self._getStopWords(language)\n",
    "        for i in palavras:\n",
    "            # print (i)\n",
    "            i.strip()    \n",
    "            if i not in listStopWords:\n",
    "                text += i+' '\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "    '''\n",
    "    A partir daqui há a possibilidade de mais de uma abordagem por conta do problema de ambiguidade\n",
    "    presente na língua portuguesa em diversos casos. Identificar o morfema pode ser uma forma eficaz\n",
    "    de fazer nossa lista de stopwords mais inteligente. Em casos como o verbo \"comer\" na primeira pessoa \n",
    "    do singular(\"como\"), um lema que pode ser importante é confundindo com uma stopword, nesse caso \n",
    "    a conjunção \"como\".\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def lemmatizePhraseWithoutStopwords(self, frase, language=\"pt-br\"):\n",
    "        # é preciso baixar o modelo --> python3 -m spacy download pt\n",
    "\n",
    "        languageAdapter = \"pt\"\n",
    "        if language != \"pt-br\":\n",
    "            languageAdapter = language\n",
    "\n",
    "        frase = self.remove_stopWords(frase)\n",
    "        nlp = spacy.load(languageAdapter)\n",
    "        frase = nlp(frase)\n",
    "        # listStopWords = self._getStopWords(language)\n",
    "        lista = [palavra.lemma_ + \":\" + palavra.pos_ for palavra in frase]\n",
    "        # lista = [palavra.lemma_ for palavra in frase for stopword in listStopWords if palavra not in stopword]\n",
    "\n",
    "        return lista\n",
    "\n",
    "    def lemmatizePhraseWithoutStopwordsandPOS(self, frase, language=\"pt-br\"):\n",
    "        # é preciso baixar o modelo --> python3 -m spacy download pt\n",
    "\n",
    "        languageAdapter = \"pt\"\n",
    "        if language != \"pt-br\":\n",
    "            languageAdapter = language\n",
    "\n",
    "        frase = self.remove_stopWords(frase)\n",
    "        # disable=['tagger']\n",
    "        nlp = spacy.load(languageAdapter, disable=['tagger'])\n",
    "        frase = nlp(frase)\n",
    "        #listStopWords = self._getStopWords(language)\n",
    "        lista = [palavra.lemma_ for palavra in frase]\n",
    "        # lista = [palavra.lemma_ for palavra in frase for stopword in listStopWords if palavra not in stopword]\n",
    "\n",
    "        return lista\n",
    "        \n",
    "    def lemmatizePhrase(self, frase, language=\"pt\"):\n",
    "        # é preciso baixar o modelo --> python3 -m spacy download pt\n",
    "        nlp = spacy.load('pt')\n",
    "        frase = nlp(frase)\n",
    "\n",
    "        lista = [palavra.lemma_ for palavra in frase]\n",
    "\n",
    "        return lista\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de uso \n",
    "\n",
    "\n",
    "Tente rodar na sua máquina!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'removeStopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-797f6e67380d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Eu como aquela batata como os ingleses comem aquela batata sintática.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_stopWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatizePhraseWithoutStopwordsandPOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d7ca13fd0bf5>\u001b[0m in \u001b[0;36mremove_stopWords\u001b[0;34m(self, tweet, language, list_stopWords)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mlistStopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_stopWords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mlistStopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getStopWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mlistStopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_stopWords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d7ca13fd0bf5>\u001b[0m in \u001b[0;36m_getStopWords\u001b[0;34m(self, language)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopWords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoveStopWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistStopWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopWords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'removeStopWords' is not defined"
     ]
    }
   ],
   "source": [
    "from Preprocessor import PreProcessor\n",
    "\n",
    "prep = PreProcessor()\n",
    "\n",
    "frase = \"Eu como aquela batata como os ingleses comem aquela batata sintática.\"\n",
    "k = prep.remove_stopWords(frase)\n",
    "print(k)\n",
    "w = prep.lemmatizePhraseWithoutStopwordsandPOS(frase)\n",
    "print(w)\n",
    "z = prep.lemmatizePhraseWithoutStopwords(frase)\n",
    "print(z)\n",
    "j = prep.lemmatizePhrase(frase)\n",
    "print(j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
